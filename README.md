# Attention with Linear Biases
These are notes for a Paper Club meeting that I lead on 2023/08/09

The discussion is about the `Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation` paper

Related Papers: 
	`Train Short, Test Long` https://arxiv.org/abs/2108.12409
	`Lost in the Middle` https://arxiv.org/abs//2307.03172

## Discussion Goals
What is the big idea?
What are the implications?
Anything that the club doesn't understand?

Other interests:
- How to [measure perplexity](https://www.youtube.com/watch?v=NURcDHhYe98&ab_channel=HuggingFace)
- ALiBi vs RoPE
- Query-Aware Contextualization

Preparation
- Use Obsidian.md to annotate papers
- Add notes about perplexity
- Research [Claude's 100k context](https://www.reddit.com/r/ChatGPTPro/comments/14zn6i1/had_claude_2_explain_the_100k_context_window_if/)
- Reference videos on the subject of ALiBi
- compare ALiBi to RoPE
